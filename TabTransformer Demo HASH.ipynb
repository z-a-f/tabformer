{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fecca3-4cd0-4f8c-aa3f-6cf60f9c2070",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "## Get the repo\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/z-a-f/tabformer.git\n",
    "cd tabformer\n",
    "git submodule update --init --recursive\n",
    "git submodule sync\n",
    "```\n",
    "\n",
    "## Install the prerequisites\n",
    "\n",
    "```shell\n",
    "conda install numpy, pandas, einops -c conda-forge\n",
    "conda install transformers, datasets\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c609f73b-33fd-4292-bce5-c2d06b77ba4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(os.path.abspath('third-party/tab-transformer/tab_transformer_pytorch'))\n",
    "\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "demo_data_path = 'demo_data'\n",
    "csv_path = os.path.join(demo_data_path, 'data.csv')\n",
    "\n",
    "os.makedirs(demo_data_path, exist_ok=True)\n",
    "\n",
    "# All columns\n",
    "all_columns = [\n",
    "    'page_protocol',\n",
    "    'page_host',\n",
    "    'api_protocol',\n",
    "    'api_host',\n",
    "    'api_path',\n",
    "    'api_method',\n",
    "    'psm',\n",
    "    'scope',\n",
    "    'field_name',\n",
    "    'field_path',\n",
    "    'field_sample',\n",
    "    'data_type_name',\n",
    "    'some_cat_column',\n",
    "    'some_num_column',\n",
    "    'some_float_column',\n",
    "]\n",
    "\n",
    "# Hash size as value\n",
    "categorical_hash_columns = {\n",
    "    'page_protocol': 128,\n",
    "    'page_host': 128,\n",
    "    'api_protocol': 128,\n",
    "    'api_host': 128,\n",
    "    'api_path': 128,\n",
    "    'api_method': 128,\n",
    "    'psm': 128,\n",
    "    'scope': 128,\n",
    "    'field_name': 128,\n",
    "    'field_path': 128,\n",
    "    'field_sample': 128,\n",
    "}\n",
    "\n",
    "# Number of categories as value\n",
    "categorical_columns = [\n",
    "    'some_cat_column',\n",
    "]\n",
    "\n",
    "numerical_columns = [\n",
    "    'some_num_column',\n",
    "    'some_float_column',\n",
    "]\n",
    "\n",
    "target_column = ('data_type_name', 32)  # Maximum number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d533005c-a26f-4df0-84f3-d9ca1a8d8a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_protocol</th>\n",
       "      <th>page_host</th>\n",
       "      <th>api_protocol</th>\n",
       "      <th>api_host</th>\n",
       "      <th>api_path</th>\n",
       "      <th>api_method</th>\n",
       "      <th>psm</th>\n",
       "      <th>scope</th>\n",
       "      <th>field_name</th>\n",
       "      <th>field_path</th>\n",
       "      <th>field_sample</th>\n",
       "      <th>data_type_name</th>\n",
       "      <th>some_cat_column</th>\n",
       "      <th>some_num_column</th>\n",
       "      <th>some_float_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>/api/v1/product/prohibited/words/check</td>\n",
       "      <td>post</td>\n",
       "      <td>oec.product.product_api</td>\n",
       "      <td>query</td>\n",
       "      <td>browser_language</td>\n",
       "      <td>browser_language</td>\n",
       "      <td>en-US</td>\n",
       "      <td>Language</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>/api/v1/product/prohibited/words/check</td>\n",
       "      <td>post</td>\n",
       "      <td>oec.product.product_api</td>\n",
       "      <td>query</td>\n",
       "      <td>browser_language</td>\n",
       "      <td>browser_language</td>\n",
       "      <td>en-US</td>\n",
       "      <td>Language</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>/api/v1/product/prohibited/words/check</td>\n",
       "      <td>post</td>\n",
       "      <td>oec.product.product_api</td>\n",
       "      <td>query</td>\n",
       "      <td>browser_language</td>\n",
       "      <td>browser_language</td>\n",
       "      <td>en-US</td>\n",
       "      <td>Language</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>https</td>\n",
       "      <td>open-api.example.com</td>\n",
       "      <td>/v0/oauth/check_qr</td>\n",
       "      <td>get</td>\n",
       "      <td>abcd.openapi.gateway</td>\n",
       "      <td>query</td>\n",
       "      <td>token</td>\n",
       "      <td>token</td>\n",
       "      <td>abcd_aueast3</td>\n",
       "      <td>Account Setting</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>https</td>\n",
       "      <td>open-api.example.com</td>\n",
       "      <td>/v0/oauth/check_qr</td>\n",
       "      <td>get</td>\n",
       "      <td>abcd.openapi.gateway</td>\n",
       "      <td>query</td>\n",
       "      <td>token</td>\n",
       "      <td>token</td>\n",
       "      <td>wxyz_aueast3</td>\n",
       "      <td>Account Setting</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>/api/v1/product/list/seller/warehouses</td>\n",
       "      <td>get</td>\n",
       "      <td>oec.product.product_api</td>\n",
       "      <td>query</td>\n",
       "      <td>browser_version</td>\n",
       "      <td>browser_version</td>\n",
       "      <td>5.0%20%28Windows%20NT%2010.0%3B%20Win64%3B%20x...</td>\n",
       "      <td>User Agent</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>https</td>\n",
       "      <td>market-au.example.com</td>\n",
       "      <td>/api/v1/seller/holiday_mode/list</td>\n",
       "      <td>get</td>\n",
       "      <td>oec.seller.profile_api</td>\n",
       "      <td>query</td>\n",
       "      <td>locale</td>\n",
       "      <td>locale</td>\n",
       "      <td>en</td>\n",
       "      <td>Language</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  page_protocol              page_host api_protocol               api_host  \\\n",
       "0         https  market-au.example.com        https  market-au.example.com   \n",
       "1         https  market-au.example.com        https  market-au.example.com   \n",
       "2         https  market-au.example.com        https  market-au.example.com   \n",
       "3         https  market-au.example.com        https   open-api.example.com   \n",
       "4         https  market-au.example.com        https   open-api.example.com   \n",
       "5         https  market-au.example.com        https  market-au.example.com   \n",
       "6         https  market-au.example.com        https  market-au.example.com   \n",
       "\n",
       "                                 api_path api_method                      psm  \\\n",
       "0  /api/v1/product/prohibited/words/check       post  oec.product.product_api   \n",
       "1  /api/v1/product/prohibited/words/check       post  oec.product.product_api   \n",
       "2  /api/v1/product/prohibited/words/check       post  oec.product.product_api   \n",
       "3                      /v0/oauth/check_qr        get     abcd.openapi.gateway   \n",
       "4                      /v0/oauth/check_qr        get     abcd.openapi.gateway   \n",
       "5  /api/v1/product/list/seller/warehouses        get  oec.product.product_api   \n",
       "6        /api/v1/seller/holiday_mode/list        get   oec.seller.profile_api   \n",
       "\n",
       "   scope        field_name        field_path  \\\n",
       "0  query  browser_language  browser_language   \n",
       "1  query  browser_language  browser_language   \n",
       "2  query  browser_language  browser_language   \n",
       "3  query             token             token   \n",
       "4  query             token             token   \n",
       "5  query   browser_version   browser_version   \n",
       "6  query            locale            locale   \n",
       "\n",
       "                                        field_sample   data_type_name  \\\n",
       "0                                              en-US         Language   \n",
       "1                                              en-US         Language   \n",
       "2                                              en-US         Language   \n",
       "3                                       abcd_aueast3  Account Setting   \n",
       "4                                       wxyz_aueast3  Account Setting   \n",
       "5  5.0%20%28Windows%20NT%2010.0%3B%20Win64%3B%20x...       User Agent   \n",
       "6                                                 en         Language   \n",
       "\n",
       "   some_cat_column  some_num_column  some_float_column  \n",
       "0                1                3               -0.3  \n",
       "1                2                2               -0.2  \n",
       "2                1                1               -0.1  \n",
       "3                3                0                0.0  \n",
       "4                1                1                0.2  \n",
       "5                5                2                0.4  \n",
       "6                8                3                0.6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(csv_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1e4dea3f-6e62-49c3-95b3-f4a2f8f86246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[220, 104, 220, 104,  86, 159,  69, 164,  42,  42,  35,   1],\n",
       "         [220, 104, 220, 104,  86, 159,  69, 164,  42,  42,  35,   2],\n",
       "         [220, 104, 220, 104,  86, 159,  69, 164,  42,  42,  35,   1],\n",
       "         [220, 104, 220,  34, 201, 124, 136, 164,  94,  94, 177,   3],\n",
       "         [220, 104, 220,  34, 201, 124, 136, 164,  94,  94, 164,   1],\n",
       "         [220, 104, 220, 104, 191, 124,  69, 164, 153, 153, 160,   4],\n",
       "         [220, 104, 220, 104,  52, 124,  72, 164, 136, 136,  46,   5]]),\n",
       " tensor([[ 3.0000, -0.3000],\n",
       "         [ 2.0000, -0.2000],\n",
       "         [ 1.0000, -0.1000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 1.0000,  0.2000],\n",
       "         [ 2.0000,  0.4000],\n",
       "         [ 3.0000,  0.6000]]),\n",
       " tensor([0, 0, 0, 1, 1, 2, 0]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility functions\n",
    "import copy\n",
    "import hashlib\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self,\n",
    "                 categorical_hash_columns=None,\n",
    "                 categorical_columns=None,\n",
    "                 numerical_columns=None,\n",
    "                 target_columns=None,\n",
    "                 shared_categories=False):\n",
    "        # We will be accessing by index, which means the keys don't matter\n",
    "        self.categorical_hash_columns = categorical_hash_columns\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.numerical_columns = numerical_columns\n",
    "        self.target_columns = target_columns\n",
    "        self.shared_categories = shared_categories\n",
    "\n",
    "        self.hash_functions = [\n",
    "            np.vectorize(lambda value: self._hash_fn(value, hash_mod))\n",
    "            for hash_mod in self.categorical_hash_columns.values()\n",
    "        ]\n",
    "\n",
    "        self.target_mapping = {}\n",
    "        self.target_indexing = []\n",
    "\n",
    "        # Need to fit the data\n",
    "        self.UNK = '<UNK>'\n",
    "\n",
    "        if self.shared_categories:\n",
    "            self.cat_mapping = {self.UNK: 0}\n",
    "            self.cat_indexing = [self.UNK]\n",
    "            self.cat_functions = np.vectorize(lambda value: self.cat_mapping.get(value, self.cat_mapping[self.UNK]))\n",
    "        else:\n",
    "            self.cat_mapping = [\n",
    "                {self.UNK: 0}\n",
    "                for _ in range(len(categorical_columns))\n",
    "            ]\n",
    "            self.cat_indexing = [\n",
    "                [self.UNK]\n",
    "                for _ in range(len(categorical_columns))\n",
    "            ]\n",
    "            self.cat_functions = [\n",
    "                np.vectorize(lambda value: self.cat_mapping[idx].get(value, self.cat_mapping[idx][self.UNK]))\n",
    "                for idx in range(len(categorical_columns))\n",
    "            ]\n",
    "\n",
    "    def __call__(self, hash_data, cat_data, num_data, target_data=None):\n",
    "        hash_data, cat_data, num_data, target_data = self.tokenize(\n",
    "            hash_data=hash_data, cat_data=cat_data,\n",
    "            num_data=num_data, target_data=target_data\n",
    "        )\n",
    "        X_cats = np.hstack([hash_data, cat_data])\n",
    "        X_nums = num_data\n",
    "        y = target_data\n",
    "        return self.to_tensor(X_cats, X_nums, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_tensor(X_cat, X_num, y=None):\n",
    "        X_cat_tensor = torch.tensor(X_cat, dtype=torch.long)\n",
    "        X_num_tensor = torch.tensor(X_num, dtype=torch.float32)\n",
    "    \n",
    "        if y is not None:\n",
    "            y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "            if y_tensor.ndim == 2 and y_tensor.shape[1] == 1:\n",
    "                y_tensor = y_tensor.flatten()\n",
    "            if y_tensor.ndim != 1:\n",
    "                raise ValueError(f'Multitarget is not supported yet')\n",
    "        \n",
    "            return X_cat_tensor, X_num_tensor, y_tensor\n",
    "        else:\n",
    "            return X_cat_tensor, X_num_tensor\n",
    "                                            \n",
    "    def tokenize(self, *, hash_data=None, cat_data=None, num_data=None, target_data=None):\n",
    "        return (\n",
    "            self.tokenize_hash(hash_data) if hash_data is not None else None,\n",
    "            self.tokenize_cats(cat_data) if cat_data is not None else None,\n",
    "            num_data.to_numpy() if isinstance(num_data, pd.DataFrame) else num_data,\n",
    "            self.tokenize_target(target_data) if target_data is not None else None\n",
    "        )\n",
    "\n",
    "    def tokenize_hash(self, data):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.to_numpy()\n",
    "        assert data.shape[1] == len(self.categorical_hash_columns)\n",
    "        result = np.vstack([\n",
    "            self.hash_functions[idx](data[:, idx])\n",
    "            for idx in range(data.shape[1])\n",
    "        ]).T\n",
    "        return result\n",
    "\n",
    "    def tokenize_cats(self, data):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.to_numpy()\n",
    "        assert data.shape[1] == len(self.categorical_columns)\n",
    "        if self.shared_categories:\n",
    "            result = [\n",
    "                self.cat_functions(data[:, idx])\n",
    "                for idx in range(data.shape[1])\n",
    "            ]\n",
    "        else:\n",
    "            result = [\n",
    "                self.cat_functions[idx](data[:, idx])\n",
    "                for idx in range(data.shape[1])\n",
    "            ]\n",
    "        return np.vstack(result).T\n",
    "\n",
    "    def tokenize_target(self, target):\n",
    "        if isinstance(target, pd.DataFrame):\n",
    "            target = target.to_numpy()\n",
    "        return np.vectorize(lambda value: self.target_mapping[value])(target).reshape(target.shape)\n",
    "\n",
    "    def decode_target(self, target_tokens):\n",
    "        return np.vectorize(lambda value: self.target_indexing[value])(target_tokens).reshape(target_tokens.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _hash_fn(value: str, hash_mod: int):\n",
    "        value = str(value).encode()\n",
    "        # Applies hash to a single string\n",
    "        # Using multiple different hash algorithms reduces the collision rate\n",
    "        value = ( int(hashlib.sha1(value).hexdigest(), 16) % hash_mod\n",
    "                + int(hashlib.md5(value).hexdigest(), 16) % hash_mod\n",
    "                + int(hashlib.sha256(value).hexdigest(), 16) % hash_mod\n",
    "        ) % hash_mod\n",
    "        return value\n",
    "\n",
    "    def fit_categorical(self, data):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.to_numpy()\n",
    "        if self.shared_categories:\n",
    "            unique_values = set(np.unique(data))\n",
    "            current_tokens = set(self.cat_indexing)\n",
    "            new_tokens = unique_values - current_tokens\n",
    "            self.cat_indexing.extend(new_tokens)\n",
    "            self.cat_mapping = {key: idx for idx, key in enumerate(self.cat_indexing)}\n",
    "        else:\n",
    "            unique_values = [set(np.unique(data[:, idx])) for idx in range(data.shape[1])]\n",
    "            current_tokens = [set(toks) for toks in self.cat_indexing]\n",
    "            new_tokens = [unique_values[idx] - current_tokens[idx] for idx in range(data.shape[1])]\n",
    "            [self.cat_indexing[idx].extend(new_tokens[idx]) for idx in range(data.shape[1])]\n",
    "            self.cat_mapping = [\n",
    "                {key: idx for ci in self.cat_indexing for idx, key in enumerate(ci)}\n",
    "            ]\n",
    "\n",
    "    def fit_target(self, targets):\n",
    "        unique_values = set(np.unique(targets))\n",
    "        new_tokens = unique_values - set(self.target_indexing)\n",
    "        self.target_indexing.extend(new_tokens)\n",
    "        self.target_mapping = {key: idx for idx, key in enumerate(self.target_indexing)}\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(hash_columns, cat_columns, num_columns, target_column[0], shared_categories=False)\n",
    "\n",
    "# Split the data into hashable, categorical, and numerical data\n",
    "hash_data = data[list(categorical_hash_columns.keys())]\n",
    "cat_data = data[list(categorical_columns)]\n",
    "num_data = data[list(numerical_columns)]\n",
    "target_data = data[target_column[0]]\n",
    "\n",
    "# Learn the categories from the categorical data\n",
    "tokenizer.fit_categorical(cat_data)\n",
    "tokenizer.fit_target(target_data)\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer(hash_data=hash_data, cat_data=cat_data, num_data=num_data, target_data=target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223619d-449c-470f-9bca-4b61bf46d7f5",
   "metadata": {},
   "source": [
    "# Step 0. Load the CSV file + Statistical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6ff3d2ee-0502-4438-8e3b-3b09a59f1b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640285f1-5dfe-4121-9b08-4b46de8c56a2",
   "metadata": {},
   "source": [
    "# Step 1. Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e8b778c7-f69d-4e44-bd9a-1492aa5deb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric batch\n",
      "{'X_cat': tensor([[220, 104, 220, 104, 191, 124,  69, 164, 153, 153, 160,   4],\n",
      "        [220, 104, 220,  34, 201, 124, 136, 164,  94,  94, 164,   1],\n",
      "        [220, 104, 220, 104,  52, 124,  72, 164, 136, 136,  46,   5],\n",
      "        [220, 104, 220,  34, 201, 124, 136, 164,  94,  94, 177,   3],\n",
      "        [220, 104, 220, 104,  86, 159,  69, 164,  42,  42,  35,   1],\n",
      "        [220, 104, 220, 104,  86, 159,  69, 164,  42,  42,  35,   2],\n",
      "        [220, 104, 220, 104,  86, 159,  69, 164,  42,  42,  35,   1]]), 'X_num': tensor([[ 2.0000,  0.4000],\n",
      "        [ 1.0000,  0.2000],\n",
      "        [ 3.0000,  0.6000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 1.0000, -0.1000],\n",
      "        [ 2.0000, -0.2000],\n",
      "        [ 3.0000, -0.3000]]), 'y': tensor([2, 1, 0, 1, 0, 0, 0])}\n",
      "DataFrame for the same numeric values\n"
     ]
    }
   ],
   "source": [
    "class TabularCSVDataset(torch.utils.data.Dataset):\n",
    "    r'''PyTorch dataset to load CSV files\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path for the CSV file\n",
    "        tokenizer: Tokenizer that is used to tokenize the data\n",
    "        transform: Transformation callable to apply to the data before returning\n",
    "\n",
    "    Methods:\n",
    "        __getitem__:\n",
    "            Returns a dict with keys 'X_cat', 'X_num', 'y_cat'\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> dataset = TabularCSVDataset(csv_path, meta_path)\n",
    "        >>> batch = dataset[:10]\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 csv_path,\n",
    "                 tokenizer,\n",
    "                 transform=None):\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "        # We don't have to open the CSV -- we can just load the data iteratively\n",
    "        # Load the data\n",
    "        data = pd.read_csv(self.csv_path)\n",
    "        # Separate the data into columns\n",
    "        data_hash = data[list(tokenizer.categorical_hash_columns.keys())]\n",
    "        data_cats = data[list(tokenizer.categorical_columns)]\n",
    "        data_nums = data[list(tokenizer.numerical_columns)]\n",
    "        data_target = data[tokenizer.target_columns]\n",
    "\n",
    "        self.X_cat, self.X_num, self.y = self.tokenizer(\n",
    "            hash_data=data_hash, cat_data=data_cats, num_data=data_nums, target_data=data_target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {'X_cat': self.X_cat[idx], 'X_num': self.X_num[idx], 'y': self.y[idx]}\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n",
    "\n",
    "    \n",
    "\n",
    "training_set = TabularCSVDataset(csv_path, tokenizer)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=512, shuffle=True)\n",
    "\n",
    "batch = next(iter(training_loader))\n",
    "# batch_df = training_set.from_numeric(**batch)\n",
    "\n",
    "print(f'Numeric batch')\n",
    "print(batch)\n",
    "print(f'DataFrame for the same numeric values')\n",
    "# batch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d4606-9f09-43ea-9814-bdf1e7d1df63",
   "metadata": {},
   "source": [
    "# Step 3. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c3e8a93e-a356-427c-b7ad-1d685137db58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabTransformer(\n",
       "  (category_embed): Embedding(2824, 28)\n",
       "  (norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "  (transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=32, out_features=288, bias=False)\n",
       "            (to_out): Linear(in_features=96, out_features=32, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (1): GEGLU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=386, out_features=1544, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1544, out_features=772, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=772, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters from the original paper\n",
    "architecture_params = {\n",
    "    'dim': 32,\n",
    "    'depth': 6,\n",
    "    'heads': 6,\n",
    "    'attn_dropout': 0.1,\n",
    "    'ff_dropout': 0.1,\n",
    "    'mlp_hidden_mults': (4, 2),\n",
    "    'mlp_act': nn.ReLU(),  # Can be reused, as this is stateless\n",
    "    'use_shared_categ_embed': True,\n",
    "}\n",
    "\n",
    "data_params = {\n",
    "    # [Hash, Cat]\n",
    "    'categories': [num_cats for num_cats in tokenizer.categorical_hash_columns.values()] + ([len(cm) for cm in tokenizer.cat_mapping]),\n",
    "    'num_continuous': len(tokenizer.numerical_columns),\n",
    "    'dim_out': len(tokenizer.target_mapping)\n",
    "}\n",
    "\n",
    "model = TabTransformer(\n",
    "    **data_params,\n",
    "    **architecture_params,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8a5593ab-260b-4815-a677-1c64c5f3c412",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model number of parameters: 2021855\n",
      "Model size: 8.12 Mb\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Compute rough number of parameters\n",
    "num_parameters = 0\n",
    "for param in model.parameters():\n",
    "    num_parameters += param.numel()\n",
    "print(f'Model number of parameters: {num_parameters}')\n",
    "\n",
    "# Get the model size, as saved\n",
    "model_path = Path('/tmp/tab-transformer-temp.pt')\n",
    "torch.save(model.cpu().state_dict(), model_path)\n",
    "model_size_mb = model_path.stat().st_size / 1_000_000\n",
    "print(f'Model size: {model_size_mb:.2f} Mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e52ec90-ae36-4b21-8a5d-bc0e03fc5373",
   "metadata": {},
   "source": [
    "# Step 4. Train the model\n",
    "\n",
    "**Note** We will be doing the supervised learning instead of unsupervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e14cf6ee-46fd-4c9a-9428-73a9e6594764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([1, 1, 0, 0, 2, 0, 0])\n",
      "predictions=['Account Setting', 'Account Setting', 'Account Setting', 'Account Setting', 'Account Setting', 'Account Setting', 'Account Setting']\n",
      "expectations=['Account Setting', 'Account Setting', 'Language', 'Language', 'User Agent', 'Language', 'Language']\n"
     ]
    }
   ],
   "source": [
    "# Random output\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "# model(X_cat_tensor, X_num_tensor)\n",
    "\n",
    "batch = next(iter(training_loader))\n",
    "with torch.no_grad():\n",
    "    X_cat, X_num = batch['X_cat'], batch['X_num']\n",
    "    X_cat = X_cat.to(device)\n",
    "    X_num = X_num.to(device)\n",
    "    y_hat = model(X_cat, X_num)\n",
    "    y_hat = y_hat.argmax(-1)\n",
    "\n",
    "    print(y_hat, batch['y'])\n",
    "\n",
    "    predictions = tokenizer.decode_target(y_hat.cpu()).tolist()\n",
    "    expectations = tokenizer.decode_target(batch['y'].cpu()).tolist()\n",
    "\n",
    "\n",
    "print(f'{predictions=}')\n",
    "print(f'{expectations=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4f079ec2-7137-4d11-90e2-f2ecfabf9834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in training_loader:\n",
    "        X_cat_tensor = batch['X_cat'].to(device)\n",
    "        X_num_tensor = batch['X_num'].to(device)\n",
    "        y_tensor = batch['y'].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_hat = model(X_cat_tensor, X_num_tensor)\n",
    "        loss = criterion(y_hat, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e5fa5-6324-49ed-bce6-6a6de5741f3e",
   "metadata": {},
   "source": [
    "# Step 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "31252cc0-5f95-444d-9908-ceaa1ebf42ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1, 0, 2], device='cuda:0') tensor([0, 0, 0, 1, 1, 0, 2])\n",
      "predictions=['Language', 'Language', 'Language', 'Account Setting', 'Account Setting', 'Language', 'User Agent']\n",
      "expectations=['Language', 'Language', 'Language', 'Account Setting', 'Account Setting', 'Language', 'User Agent']\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(training_loader))\n",
    "with torch.no_grad():\n",
    "    X_cat, X_num = batch['X_cat'], batch['X_num']\n",
    "    X_cat = X_cat.to(device)\n",
    "    X_num = X_num.to(device)\n",
    "    y_hat = model(X_cat, X_num)\n",
    "    y_hat = y_hat.argmax(-1)\n",
    "\n",
    "    print(y_hat, batch['y'])\n",
    "\n",
    "    predictions = tokenizer.decode_target(y_hat.cpu()).tolist()\n",
    "    expectations = tokenizer.decode_target(batch['y'].cpu()).tolist()\n",
    "\n",
    "\n",
    "print(f'{predictions=}')\n",
    "print(f'{expectations=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb52060-09c5-49e8-9a7a-949c4d2785a3",
   "metadata": {},
   "source": [
    "# Step 6. Save the trained model (Pure Torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "853e8018-f0e0-4ba2-aace-8845513d0aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = Path('/tmp/tab-transformer-temp.pt')  # This is where the model will be saved to\n",
    "torch.save(model.cpu(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fa26e-6509-4e11-9172-1f0d5f9182c3",
   "metadata": {},
   "source": [
    "# Step 7. Load the model from the pretrained version (Pure Torch)\n",
    "\n",
    "This step will be done on target machine (inference server, edge device, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8699da03-32da-4e58-a5cb-b22fb190005f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 0, 0, 0, 1], device='cuda:0') tensor([0, 1, 2, 0, 0, 0, 1])\n",
      "predictions=['Language', 'Account Setting', 'User Agent', 'Language', 'Language', 'Language', 'Account Setting']\n",
      "expectations=['Language', 'Account Setting', 'User Agent', 'Language', 'Language', 'Language', 'Account Setting']\n"
     ]
    }
   ],
   "source": [
    "inference_model = torch.load(save_path).to(device)\n",
    "\n",
    "batch = next(iter(training_loader))\n",
    "with torch.no_grad():\n",
    "    X_cat, X_num = batch['X_cat'], batch['X_num']\n",
    "    X_cat = X_cat.to(device)\n",
    "    X_num = X_num.to(device)\n",
    "    y_hat = inference_model(X_cat, X_num)\n",
    "    y_hat = y_hat.argmax(-1)\n",
    "\n",
    "    print(y_hat, batch['y'])\n",
    "\n",
    "    predictions = tokenizer.decode_target(y_hat.cpu()).tolist()\n",
    "    expectations = tokenizer.decode_target(batch['y'].cpu()).tolist()\n",
    "\n",
    "\n",
    "print(f'{predictions=}')\n",
    "print(f'{expectations=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285dbca-26f1-47ed-8ee0-d92442f89ab0",
   "metadata": {},
   "source": [
    "# JIT PyTorch\n",
    "\n",
    "* This model does not support JIT scripting\n",
    "* This model does not support JIT tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c8080-1ddf-4bea-925e-3502f276a632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
